{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phBcEcokOogy"
      },
      "source": [
        "# Tracing for Different Types of Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6c7WCRxOogz"
      },
      "source": [
        "### Types of Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiPQbjriOog0"
      },
      "source": [
        "LangSmith supports many different types of Runs - you can specify what type your Run is in the @traceable decorator. The types of runs are:\n",
        "\n",
        "- LLM: Invokes an LLM\n",
        "- Retriever: Retrieves documents from databases or other sources\n",
        "- Tool: Executes actions with function calls\n",
        "- Chain: Default type; combines multiple Runs into a larger process\n",
        "- Prompt: Hydrates a prompt to be used with an LLM\n",
        "- Parser: Extracts structured data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bqjy4qeOog0"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlRgBjkFOog0"
      },
      "outputs": [],
      "source": [
        "# You can set them inline!\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvGysrGEOog1"
      },
      "source": [
        "### LLM Runs for Chat Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmIcfpASOog1"
      },
      "source": [
        "LangSmith provides special rendering and processing for LLM traces. In order to make the most of this feature, you must log your LLM traces in a specific format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4HuN4FaOog1"
      },
      "source": [
        "For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content.\n",
        "\n",
        "The output is accepted in any of the following formats:\n",
        "\n",
        "- A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.\n",
        "- A dictionary/object that contains the key message with a value that is a message object with the keys role and content.\n",
        "- A tuple/array of two elements, where the first element is the role and the second element is the content.\n",
        "- A dictionary/object that contains the key role and content.\n",
        "The input to your function should be named messages.\n",
        "\n",
        "You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly.\n",
        "- ls_provider: The provider of the model, eg \"openai\", \"anthropic\", etc.\n",
        "- ls_model_name: The name of the model, eg \"gpt-4o-mini\", \"claude-3-opus-20240307\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_vChFbxOog1",
        "outputId": "684d2f62-bc56-4452-8041-69957d97ed2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'choices': [{'message': {'role': 'assistant',\n",
              "    'content': 'Sure, what time would you like to book the table for?'}}]}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "inputs = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "  {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},\n",
        "]\n",
        "\n",
        "output = {\n",
        "  \"choices\": [\n",
        "      {\n",
        "          \"message\": {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": \"Sure, what time would you like to book the table for?\"\n",
        "          }\n",
        "      }\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Can also use one of:\n",
        "# output = {\n",
        "#     \"message\": {\n",
        "#         \"role\": \"assistant\",\n",
        "#         \"content\": \"Sure, what time would you like to book the table for?\"\n",
        "#     }\n",
        "# }\n",
        "#\n",
        "# output = {\n",
        "#     \"role\": \"assistant\",\n",
        "#     \"content\": \"Sure, what time would you like to book the table for?\"\n",
        "# }\n",
        "#\n",
        "# output = [\"assistant\", \"Sure, what time would you like to book the table for?\"]\n",
        "\n",
        "@traceable(\n",
        "  # TODO: Add an run_type=\"llm\", and metadata for ls_provider, and ls_model_name\n",
        ")\n",
        "def chat_model(messages: list):\n",
        "  return output\n",
        "\n",
        "chat_model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQiPEYubOog1"
      },
      "source": [
        "### Handling Streaming LLM Runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e29R7yCvOog2"
      },
      "source": [
        "For streaming, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LEIN534Oog2",
        "outputId": "3ec55f3d-cd9a-4027-8103-5e575f6c7d35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'choices': [{'message': {'content': 'Hello, polly the parrot',\n",
              "     'role': 'assistant'}}]}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def _reduce_chunks(chunks: list):\n",
        "    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\n",
        "    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\n",
        "\n",
        "@traceable(\n",
        "    run_type=\"llm\",\n",
        "    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"},\n",
        "    # TODO: Add a reduce_fn\n",
        ")\n",
        "def my_streaming_chat_model(messages: list):\n",
        "    for chunk in [\"Hello, \" + messages[1][\"content\"]]:\n",
        "        yield {\n",
        "            \"choices\": [\n",
        "                {\n",
        "                    \"message\": {\n",
        "                        \"content\": chunk,\n",
        "                        \"role\": \"assistant\",\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "list(\n",
        "    my_streaming_chat_model(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},\n",
        "            {\"role\": \"user\", \"content\": \"polly the parrot\"},\n",
        "        ],\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaDs9-2AOog2"
      },
      "source": [
        "### Retriever Runs + Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hofr2orZOog2"
      },
      "source": [
        "Many LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever. LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken.\n",
        "\n",
        "1. Annotate the retriever step with run_type=\"retriever\".\n",
        "2. Return a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:\n",
        "    - page_content: The text of the document.\n",
        "    - type: This should always be \"Document\".\n",
        "    - metadata: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8E0GNLTOog2",
        "outputId": "44b3b334-692a-408c-e2ad-d1aeb5ae7b70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'page_content': 'Document contents 1',\n",
              "  'type': 'Document',\n",
              "  'metadata': {'foo': 'bar'}},\n",
              " {'page_content': 'Document contents 2',\n",
              "  'type': 'Document',\n",
              "  'metadata': {'foo': 'bar'}},\n",
              " {'page_content': 'Document contents 3',\n",
              "  'type': 'Document',\n",
              "  'metadata': {'foo': 'bar'}}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "def _convert_docs(results):\n",
        "  return [\n",
        "      {\n",
        "          \"page_content\": r,\n",
        "          \"type\": \"Document\", # This is the wrong format! The key should be type\n",
        "          \"metadata\": {\"foo\": \"bar\"}\n",
        "      }\n",
        "      for r in results\n",
        "  ]\n",
        "\n",
        "@traceable(\n",
        "    # TODO: Add an run_type=\"retriever\"\n",
        ")\n",
        "def retrieve_docs(query):\n",
        "  # Retriever returning hardcoded dummy documents.\n",
        "  # In production, this could be a real vector datatabase or other document index.\n",
        "  contents = [\"Document contents 1\", \"Document contents 2\", \"Document contents 3\"]\n",
        "  return _convert_docs(contents)\n",
        "\n",
        "retrieve_docs(\"User query\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXC0ioteOog2"
      },
      "source": [
        "### Tool Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFz6VoMsOog2"
      },
      "source": [
        "LangSmith has custom rendering for Tool Calls made by the model to make it clear when provided tools are being used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R39y2UypPe_8",
        "outputId": "0998d8f5-9549-426c-a87d-c40c6e61a1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.32.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Downloading groq-0.32.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.32.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV6T7oQOOog2",
        "outputId": "614700ae-33b4-4117-fdef-1a7a4463385f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-c255d8cf-fdb1-45ba-a27c-dd8bb42d4e54', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current temperature in New\\u202fYork City is **65\\u202f°F**.', role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1759341348, model='openai/gpt-oss-120b', object='chat.completion', system_fingerprint='fp_ed9190d8b7', usage=CompletionUsage(completion_tokens=19, prompt_tokens=224, total_tokens=243, completion_time=0.039831769, prompt_time=0.015450292, queue_time=0.530931146, total_time=0.055282061), usage_breakdown=None, x_groq={'id': 'req_01k6gen1h8fed9a3p522csyhd6'}, service_tier='on_demand')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "from groq import Groq\n",
        "from typing import List, Optional\n",
        "import json\n",
        "\n",
        "# Groq client\n",
        "groq_client = Groq()\n",
        "\n",
        "@traceable(run_type=\"tool\")\n",
        "def get_current_temperature(location: str, unit: str):\n",
        "    return 65 if unit == \"Fahrenheit\" else 17\n",
        "\n",
        "@traceable(run_type=\"llm\")\n",
        "def call_groq(\n",
        "    messages: List[dict], tools: Optional[List[dict]]\n",
        ") -> str:\n",
        "    return groq_client.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-120b\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        tools=tools\n",
        "    )\n",
        "\n",
        "@traceable(run_type=\"chain\")\n",
        "def ask_about_the_weather(inputs, tools):\n",
        "    response = call_groq(inputs, tools)\n",
        "\n",
        "    # Check if the model returned tool calls\n",
        "    if response.choices[0].message.tool_calls:\n",
        "        tool_call_args = json.loads(\n",
        "            response.choices[0].message.tool_calls[0].function.arguments\n",
        "        )\n",
        "        location = tool_call_args[\"location\"]\n",
        "        unit = tool_call_args[\"unit\"]\n",
        "\n",
        "        tool_response_message = {\n",
        "            \"role\": \"tool\",\n",
        "            \"content\": json.dumps({\n",
        "                \"location\": location,\n",
        "                \"unit\": unit,\n",
        "                \"temperature\": get_current_temperature(location, unit),\n",
        "            }),\n",
        "            \"tool_call_id\": response.choices[0].message.tool_calls[0].id\n",
        "        }\n",
        "\n",
        "        inputs.append(response.choices[0].message)\n",
        "        inputs.append(tool_response_message)\n",
        "\n",
        "        output = call_groq(inputs, None)\n",
        "        return output\n",
        "    else:\n",
        "        # If no tool calls, return the initial response\n",
        "        return response\n",
        "\n",
        "\n",
        "# Tool definition\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_temperature\",\n",
        "            \"description\": \"Get the current temperature for a specific location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"Celsius\", \"Fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\", \"unit\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Example input\n",
        "inputs = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather today in New York City?\"},\n",
        "]\n",
        "\n",
        "ask_about_the_weather(inputs, tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZMGiaKRQB4X",
        "outputId": "7f7a5c1a-7338-49ff-d1c5-77b0d7c42d1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The current temperature in New York City is **65 °F**.\n"
          ]
        }
      ],
      "source": [
        "ai_answer = ask_about_the_weather(inputs, tools)\n",
        "print(ai_answer.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-g-6Brq9US7",
        "outputId": "0242849e-690b-4f73-b728-160fb988f452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'result': 12}\n"
          ]
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "@traceable(run_type=\"tool\", metadata={\"tool_name\": \"calculator\"})\n",
        "def add_numbers(a: int, b: int):\n",
        "    return {\"result\": a + b}\n",
        "\n",
        "print(add_numbers(5, 7))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HM6pxg__DfE"
      },
      "source": [
        "## **MY EXAMPLES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAPkx36B_IuN",
        "outputId": "10003463-0a88-4057-86c4-8ca47d9fd8d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'choices': [{'message': {'role': 'assistant', 'content': 'Sentiment: positive'}}]}\n"
          ]
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "@traceable(run_type=\"llm\", metadata={\"ls_provider\": \"mock\", \"ls_model_name\": \"sentiment_model\"})\n",
        "def analyze_sentiment(text: str):\n",
        "    sentiment = \"positive\" if \"good\" in text else \"negative\"\n",
        "    return {\"choices\": [{\"message\": {\"role\": \"assistant\", \"content\": f\"Sentiment: {sentiment}\"}}]}\n",
        "\n",
        "print(analyze_sentiment(\"The movie was really good!\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P3aRmg9AOmz",
        "outputId": "51106f63-cb43-44ad-f7ee-191a780116c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'choices': [{'message': {'role': 'assistant', 'content': \"Entities: ['Paris', 'France']\"}}]}\n"
          ]
        }
      ],
      "source": [
        "@traceable(run_type=\"llm\", metadata={\"ls_provider\": \"mock\", \"ls_model_name\": \"ner_model\"})\n",
        "def extract_entities(text: str):\n",
        "    entities = [\"Paris\", \"France\"] if \"Paris\" in text else []\n",
        "    return {\"choices\": [{\"message\": {\"role\": \"assistant\", \"content\": f\"Entities: {entities}\"}}]}\n",
        "\n",
        "print(extract_entities(\"I will travel to Paris, France next month.\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ls-academy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
