video 1 - This video shows how to create and use datasets in LangSmith for offline AI evaluation. It covers importing CSVs, adding examples manually or from traces, tagging versions, splitting datasets for specialized tests, and sharing/exporting them. The goal is to build reliable “golden sets” to track and improve app performance.

video 2 - This video explains how to create and use evaluators in LangSmith to measure AI performance with metrics like accuracy and hallucination. It covers writing custom evaluators, using LLMs as judges, running automatic evaluations, handling RAG metrics like relevance and helpfulness, tagging inputs, and applying evaluators to past runs.
